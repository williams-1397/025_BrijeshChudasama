{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import twitter_samples \r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "!pip install tensor\r\n",
    "import tensorflow.compat.v1 as tf\r\n",
    "tf.disable_v2_behavior()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensor in /usr/local/lib/python3.6/dist-packages (0.3.6)\n",
      "Requirement already satisfied: pysnmp in /usr/local/lib/python3.6/dist-packages (from tensor) (4.4.12)\n",
      "Requirement already satisfied: Twisted in /usr/local/lib/python3.6/dist-packages (from tensor) (20.3.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from tensor) (3.12.4)\n",
      "Requirement already satisfied: PyYaml in /usr/local/lib/python3.6/dist-packages (from tensor) (3.13)\n",
      "Requirement already satisfied: construct in /usr/local/lib/python3.6/dist-packages (from tensor) (2.10.56)\n",
      "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.6/dist-packages (from pysnmp->tensor) (3.9.8)\n",
      "Requirement already satisfied: pyasn1>=0.2.3 in /usr/local/lib/python3.6/dist-packages (from pysnmp->tensor) (0.4.8)\n",
      "Requirement already satisfied: pysmi in /usr/local/lib/python3.6/dist-packages (from pysnmp->tensor) (0.3.4)\n",
      "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted->tensor) (15.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from Twisted->tensor) (5.1.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted->tensor) (17.5.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted->tensor) (20.2.0)\n",
      "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted->tensor) (2.0.2)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted->tensor) (20.0.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted->tensor) (20.1.0)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->tensor) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->tensor) (49.6.0)\n",
      "Requirement already satisfied: ply in /usr/local/lib/python3.6/dist-packages (from pysmi->pysnmp->tensor) (3.11)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted->tensor) (2.10)\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "_Of77QtnJqC1",
    "outputId": "03509434-0695-45c3-9e5c-415648e02b81"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "nltk.download('twitter_samples')\r\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 3
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "xQMjaTb1KFzu",
    "outputId": "ebb440a6-8a0f-4057-c910-b8f1010c9d6f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import re\r\n",
    "import string\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "from nltk.tokenize import TweetTokenizer"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bvIZqAAHKKVU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\r\n",
    "def process_tweet(tweet):\r\n",
    "   \r\n",
    "    stemmer = PorterStemmer()\r\n",
    "    stopwords_english = stopwords.words('english')\r\n",
    "\r\n",
    "    # remove stock market tickers like $GE\r\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\r\n",
    "    # remove old style retweet text \"RT\"\r\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\r\n",
    "    # remove hyperlinks\r\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\r\n",
    " \r\n",
    "    tweet = re.sub(r'#', '', tweet)\r\n",
    "   \r\n",
    "\r\n",
    "\r\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\r\n",
    "                               reduce_len=True)\r\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\r\n",
    "\r\n",
    "    tweets_clean = []\r\n",
    "    for word in tweet_tokens:\r\n",
    "            \r\n",
    "           \r\n",
    "            # 1 remove stopwords\r\n",
    "            if(word not in stopwords_english and word not in string.punctuation):\r\n",
    "              steam_word=stemmer.stem(word)\r\n",
    "              tweets_clean.append(steam_word)\r\n",
    "            # 3 stemming word\r\n",
    "            # 4 Add it to tweets_clean\r\n",
    "    return tweets_clean"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spTGYQaBKOWp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    freqs = {}\n",
    "\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair not in freqs:\n",
    "              freqs[pair]=0\n",
    "            freqs[pair]+=1\n",
    "\n",
    "\n",
    "    return freqs"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGzzsem1Ka8E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "test_pos=all_positive_tweets[4000:]\n",
    "train_pos=all_positive_tweets[:4000]\n",
    "test_neg=all_negative_tweets[4000:]\n",
    "train_neg=all_negative_tweets[:4000]\n",
    "train_x=train_pos+train_neg\n",
    "test_x=test_pos+test_neg"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ilrmde7KfE3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oe45JUoWKlr7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "data=all_positive_tweets+all_negative_tweets\n",
    "la=np.append(np.ones((len(all_positive_tweets), 1)), np.zeros((len(all_negative_tweets), 1)), axis=0)\n",
    "train_x,test_x,train_y,test_y=train_test_split(data,la,test_size=0.30,random_state=68)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRRVtDCpKoFs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# create frequency dictionary\n",
    "\n",
    "freqs = build_freqs(train_x,train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 10312\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Q1Z1ozTGMWuI",
    "outputId": "c464869b-57da-422f-eebb-19de76bf7c0e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "#############################################################\n",
    "### ------Process tweet-----------\n",
    "# Example\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is an example of a positive tweet: \n",
      " @WillHillBet thank you for taking the time to reply, albeit with a disappointing answer - the most famous athlete racing and no odds :(\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['thank', 'take', 'time', 'repli', 'albeit', 'disappoint', 'answer', 'famou', 'athlet', 'race', 'odd', ':(']\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "Shn64UnnMyOa",
    "outputId": "b7222526-abe2-43ab-850f-effe9075065a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "#Extracting the features\n",
    "def extract_features(tweet, freqs):\n",
    " \n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 2)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    # x[0,0] = 1 \n",
    "        \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        if((word,1) in freqs):\n",
    "          x[0,0]+=freqs[word,1]\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        if((word,0) in freqs):\n",
    "          x[0,1]+=freqs[word,0]\n",
    "    \n",
    "    assert(x.shape == (1, 2))\n",
    "    return x[0]"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iC4V0eHBM8nm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# Check the function\n",
    "\n",
    "# test 1\n",
    "# test on training data\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 573. 3425.]\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Jp4L-CLENFVF",
    "outputId": "bfb2934a-b45a-4446-dbbc-33a2dec23418"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# test 2:\n",
    "# check for when the words are not in the freqs dictionary\n",
    "tmp2 = extract_features('happy', freqs)\n",
    "print(tmp2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[147.  18.]\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "s59EhEN2NM1Q",
    "outputId": "66891fcb-4518-4e01-f262-6947dc397c94"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def predict_tweet(tweet):\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "      saver.restore(sess,save_path='TSession')\n",
    "      data_i=[]\n",
    "      for t in tweet:\n",
    "        data_i.append(extract_features(t,freqs))\n",
    "      data_i=np.asarray(data_i)\n",
    "      return sess.run(tf.nn.sigmoid(tf.add(tf.matmul(a=data_i,b=W,transpose_b=True),b)))\n",
    "    print(\"Fail\")\n",
    "    return \n",
    " "
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6r92fFBNR_g"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "#Define bias and Weight\n",
    "\n",
    "b=tf.Variable(np.random.randn(1),name=\"Bias\")\n",
    "W=tf.Variable(np.random.randn(1,2),name=\"Bias\")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28W0ocxONUMO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "#Extract Feature for all tweets\n",
    "\n",
    "data=[]\n",
    "for t in train_x:\n",
    "  data.append(extract_features(t,freqs))\n",
    "data=np.asarray(data)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2zCvsjXNfaP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "#Define Sigmoid Function and Cost Function\n",
    "Y_hat = tf.nn.sigmoid(tf.add(tf.matmul(np.asarray(data), W,transpose_b=True), b)) \n",
    "print(Y_hat)\n",
    "ta=np.asarray(train_y)\n",
    "# Sigmoid Cross Entropy Cost Function \n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits( \n",
    "                    logits = Y_hat, labels = ta) \n",
    "print(cost)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor(\"Sigmoid_2:0\", shape=(7000, 1), dtype=float64)\n",
      "Tensor(\"logistic_loss_1:0\", shape=(7000, 1), dtype=float64)\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ySp5m26MNlVX",
    "outputId": "8a107488-750b-4a2c-e7d4-7dccf67edb75"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "#Define Gradient Descent Optimizer\n",
    "  \n",
    "# Gradient Descent Optimizer \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-4,name=\"GradientDescent\").minimize(cost) \n",
    "  \n",
    "# Global Variables Initializer \n",
    "init = tf.global_variables_initializer() "
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5viBpFCKNqKp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "#Run Epoch for 1000 times\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "  \n",
    "  sess.run(init)\n",
    "  print(\"Bias\",sess.run(b))\n",
    "  print(\"Weight\",sess.run(W))\n",
    "  for epoch in range(400):\n",
    "    sess.run(optimizer)\n",
    "    preds=sess.run(Y_hat)\n",
    "    acc=((preds==ta).sum())/len(train_y)\n",
    "    accu=[]\n",
    "    repoch=False\n",
    "    if repoch:\n",
    "      accu.append(acc)\n",
    "    if epoch % 1000 == 0:\n",
    "      print(\"Accuracy\",acc)\n",
    "    saved_path = saver.save(sess, 'TSession')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bias [0.89781018]\n",
      "Weight [[0.70234961 0.78590555]]\n",
      "Accuracy 0.49857142857142855\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "8IL8AJbCNyz1",
    "outputId": "4ea52681-ed74-4ab9-f852-daaaba724e96"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "#Predict All Tweet\n",
    "\n",
    "preds=predict_tweet(test_x)\n",
    "print(preds,len(test_y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Restoring parameters from TSession\n",
      "[[1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " ...\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999834]] 3000\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "eefokuSwN3y2",
    "outputId": "2b40eb42-87b1-41a1-bf03-aa809a841d96"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "#Calculate Accuracy\n",
    "\n",
    "def calculate_accuracy(x,y):\n",
    "  if len(x)!=len(y):\n",
    "    print(\"dimensions are different\")\n",
    "    return\n",
    "  return ((x==y).sum())/len(y)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psc-VIxYN8M6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "print(calculate_accuracy(preds,test_y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.492\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "87qxc9ogOE3D",
    "outputId": "dcac3d40-e00c-40e5-c10c-42d5868e9f15"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mmU05KJOGn9"
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "CE_068.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}